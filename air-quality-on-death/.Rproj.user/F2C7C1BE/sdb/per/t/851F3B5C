{
    "collab_server" : "",
    "contents" : "---\ntitle: \"Untitled\"\noutput: html_document\n---\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE)\n```\n\n### OUTPUT WITH BEST MODEL\n\nWe try to apply the best model XGBOOST to the data of the [Kaggle Boston housing competition](https://inclass.kaggle.com/c/boston-housing/)\n\n```{r}\nlibrary(caret)\ndevtools::install_github('zachmayer/caretEnsemble')\nlibrary(caretEnsemble)\nlibrary(Metrics)\nlibrary(mlbench)\ndata(BostonHousing) # only for verification\n```\n\n```{r}\nlibrary(doMC)\nregisterDoMC(cores = 3)\n```\n\n```{r}\ndata_kaggle <- read.csv(\"train_kaggle.csv\")\n\ndata_kaggle$chas <- as.factor(data_kaggle$chas)\ndata_kaggle$rad <- as.factor(data_kaggle$rad)\ndata_kaggle$ID <- NULL\n\nset.seed(123456)\n\nsplit_kaggle <- createDataPartition(data_kaggle$medv, p = 0.7, list = FALSE)\ntrain_kaggle <- data_kaggle[split_kaggle,]\ntest_kaggle <- data_kaggle[-split_kaggle,]\n\nfitControl <- trainControl(method = \"repeatedcv\",\n                           number = 10,\n                           repeats = 5,\n                           savePredictions=\"final\")\n\nxgb_grid_1 = expand.grid(\n  nrounds = 1000,\n  eta = c(0.01, 0.001, 0.0001),\n  max_depth = c(2, 4, 6, 8, 10),\n  gamma = 1,\n  colsample_bytree = 1,\n  min_child_weight = 1,\n  subsample = 1\n)\n\nnn_grid <- expand.grid(.decay = c(0.5, 0.1), .size = c(1, 2, 3, 4, 5))\n\nmodel_list_big <- caretList(\n  medv~., data=train_kaggle,\n  trControl=fitControl,\n  metric=\"RMSE\",\n  tuneList=list(\n    FIT_RPART = caretModelSpec(method=\"rpart2\", preProc=c(\"center\", \"scale\")),\n    FIT_ranger = caretModelSpec(method=\"ranger\"),\n    FIT_xgbTree = caretModelSpec(method=\"xgbTree\", tuneGrid=xgb_grid_1),\n    FIT_glmnet = caretModelSpec(method=\"glmnet\", preProc=c(\"center\", \"scale\")),\n    FIT_svmRadial = caretModelSpec(method=\"svmRadial\"),\n    FIT_knn = caretModelSpec(method=\"knn\", preProc=c(\"center\", \"scale\")),\n    FIT_nnet = caretModelSpec(method = \"nnet\", maxit = 1000, tuneGrid = nn_grid, trace = F, linout = 1, preProc=c(\"center\", \"scale\"))\n  )\n)\n\n\n\nresults <- resamples(model_list_big)\nsummary(results)\nmodelCor(results)\ncorrplot::corrplot(modelCor(results))\ndotplot(results)\n\n\n#stackControl <- trainControl(method=\"repeatedcv\", number=10, repeats=3, savePredictions=TRUE)\nstackControl <- trainControl(method=\"repeatedcv\", number=10, repeats=3, savePredictions=TRUE)\nstack_rf <- caretStack(\n              model_list_big, \n              method=\"rf\", \n              metric=\"RMSE\", \n              trControl=stackControl\n            )\nstack_glm <- caretStack(\n            model_list_big, \n            method=\"glm\", \n            metric=\"RMSE\", \n            trControl=stackControl)\n\nstack_gbm <- caretStack(\n            model_list_big, \n            method=\"gbm\",\n            verbose=FALSE, \n            metric=\"RMSE\", \n            trControl=stackControl)\n  \nstack_xgbTree <- caretStack(\n            model_list_big, \n            method=\"xgbTree\",\n            verbose=FALSE, \n            metric=\"RMSE\", \n            trControl=stackControl)\n\nstack_nn <- caretStack(\n            model_list_big, \n            method=\"nnet\",\n            verbose=FALSE, \n            metric=\"RMSE\", \n            maxit = 1000, tuneGrid = nn_grid, trace = F, linout = 1, preProc=c(\"center\", \"scale\"),\n            trControl=stackControl)\n\n\nstack_rf_pred <- predict(stack_rf, test_kaggle)\nstack_rf_rmse <- rmse(stack_rf_pred, test_kaggle$medv)\n\nstack_glm_pred <- predict(stack_glm, test_kaggle)\nstack_glm_rmse <- rmse(stack_glm_pred, test_kaggle$medv)\n\nstack_gbm_pred <- predict(stack_gbm, test_kaggle)\nstack_gbm_rmse <- rmse(stack_gbm_pred, test_kaggle$medv)\n\nstack_xgbTree_pred <- predict(stack_xgbTree, test_kaggle)\nstack_xgbTree_rmse <- rmse(stack_xgbTree_pred, test_kaggle$medv)\n\nstack_nn_pred <- predict(stack_nn, test_kaggle)\nstack_nn_rmse <- rmse(stack_nn_pred, test_kaggle$medv)\n\n# model_list_big$FIT_XGBOOST$finalModel$xNames\n\nresults2 <- caret::resamples(list(STACK_RF=stack_rf$ens_model, STACK_GLM=stack_glm$ens_model, STACK_GBM=stack_gbm$ens_model, STACK_XGBOOST=stack_xgbTree$ens_model,\n                                  STACK_NN=stack_nn$ens_model))\n\n\nsummary(results2)\ndotplot(results2)\n\n\nfit_final <- stack_rf\n\n```\n\n\n\n```{r}\noutsample_test_kaggle <- read.csv(\"test_kaggle.csv\")\n#outsample_test_kaggle <- outsample_test_kaggle[-c(413, 372, 369, 373, 402, 375, 490, 506, 215, 401),]\n\noutsample_test_kaggle$chas <- as.factor(outsample_test_kaggle$chas)\noutsample_test_kaggle$rad <- as.factor(outsample_test_kaggle$rad)\nids <- outsample_test_kaggle$ID\noutsample_test_kaggle$ID <- NULL\n\n## shorcut \n#perfectOutput <- data.frame(ID=outsample_test_kaggle$ID, medv=BostonHousing[outsample_test_kaggle$ID,\"medv\"])\n\nfit_final_pred <- predict(fit_final, outsample_test_kaggle)\noutput <- as.data.frame(cbind(ids, fit_final_pred))\ncolnames(output) <- c(\"ID\", \"medv\")\nwrite.csv(output, file = \"submission.csv\", row.names = FALSE, quote=FALSE)\n```\n\nEstimating RMSE\n```{r}\nfinal_test_rmse <- rmse(fit_final_pred, BostonHousing[ids,\"medv\"])\n```\n\n\n",
    "created" : 1490757021345.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2967056010",
    "id" : "851F3B5C",
    "lastKnownWriteTime" : 1483119012,
    "last_content_update" : 1483119012,
    "path" : "~/Dropbox/BIGDATA/_training/boston-housing/Kaggle-Boston.Rmd",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 2,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}